L'objectif de cette section est d'analyser un ensemble de méthodes pour établir des clusters sur les textes. Nous avons choisi des méthodes qui s’appuient sur le champs lexical d'une lettre, révélateur du sujet abordé. À ce jour, toutes nos méthodes utilisent 2 étapes cruciales : mettre le document sous forme vectorielle, de préférence de dimension faible \ref{vector}, puis appliquer des techniques classiques de clustering sur ces représentations vectorielles \ref{clustering}.
\section{Représentation vectorielle des documents et réduction de dimension}
\label{vector}
\subsection{Premières représentations}
\label{base}
Il est d'abbord nécessaire de partir d'un matériau de base, une première représentation vectorielle de haute dimension, avant de réduire les dimensions dans la section \ref{red}. Ces premières matrices sont sous la forme suivante : chaque ligne représente un document et chaque colonne représente un mot.
\subsubsection{TF}
La représentation la plus intuitive pour représenter le champs lexical d'un texte est la matrice Term-Frequency, constituée d'entiers indiquant combien de fois un certain mot apparaît dans un certain document.
\subsubsection{TF-IDF}
Le Term Frequency - Inverse Document Frequency ...
\subsubsection{SIF}
Le Smooth inverse Frequency ...
\subsection{Méthodes de réduction de dimension}
\subsubsection{Analyse en composante principale}
L'analyse en composante principale 
\subsubsection{Analyse Sémantique Latente}
L'analyse Sémantique Latente ... effectue une SVD , Division en Valeurs Singulières, sur la matrice des TF-IDF.
\subsubsection{Allocation de Dirichlet Latente}
L'Allocation de Dirichlet Latente (LDA) s'appuie sur la matrice des Term-Frequency. On commence par choisir la dimension $k$ qui veut représenter concrètement le nombre de thèmes pouvant être abordés. La forme vectorielle de dimension k finalement obtenue pour chaque document représentera à quel point le document correspond à chaque thème généré. Les documents sont représentés par des distributions de probabilités (\textbf{RANDOM MIXTURE}) sur les thèmes. Les thèmes sont représentés par des distributions de probabilité pour chaque mot. 
Le LDA suppose que les documents ont tous été générés de la façon suivante :

%\begin{algorithm}
%\caption{Génération de texte}
\begin{algorithmic}
\STATE Choisir le nombre de mots $N \sim$ Poissson$(\xi)$
\STATE Choisir les probabilités de chaque thème $\theta \sim$ Dir$(\alpha)$
\FOR{chaque mot $w_n$, $n$ allant de $1$ à $N$}
	\STATE Choisir un thème $Z_n \sim$ Multionomial$(\alpha,1)$
	\STATE Choisir un mot $w_n$ avec une probabilité $P(w_n | z_n , \beta)$, une probabilité multinomiale dépendante du thème $Z_n$.
\ENDFOR
\end{algorithmic}
%\end{algorithm}
$k$ est la dimension de la distribution de Dirichlet utilisée, donc du paramètre $\alpha$ et donc du nombre de thèmes possibles. $\beta$ est une matrice $V \times k$ représentant pour chaque mot la probabilité d'apparaître dans chaque thème  (avec $V$ le nombre de mots). $\alpha$ sont les paramètres, changeant pour chaque document, de laloi de Dirichlet générant $\theta$, le vecteur associant à chaque thème sa probabilité. 

On peut donc en déduire à présent une manière d'inférer ces paramètres, chose plus compliquée que nous verrons peut-être. Cela se déroule de la manière suivante : 
\begin{algorithm}
\caption{Inférence des paramètres}
\begin{algorithmic}
\STATE Initialiser les valeurs de $\theta$
\FOR{ chaque document $d$}
	\FOR{chaque mot $w$ du document}
		\STATE Calculer la probabilité $P(t|d)$ que le document $d$ soit assigné au thème $t$
		\STATE Calculer $P(w|t)$ la probabilité que le thème $t$ soit assigné au mot $w$
		\STATE On attribue alors le thème $t$ au produit de ces deux probabilités.
	\ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}
Au bout d'un moment ça converge.
\subsubsection{Factorisation en Matrices non Négatives}
Que cela signifie
\subsubsection{Uniform Manifold Approximation for Projection and Representation }
\subsection{Représentation visuelle}
\subsection{Distances}

\label{red}
\subsection{Clustering avec les représentations obtenues}
\label{clustering}
% {Document, Texte, Lettre, Donnée}
% {Méthode, technique}